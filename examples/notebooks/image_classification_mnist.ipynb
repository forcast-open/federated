{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "heavy-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflow # install tensor flow to download mnist dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-board",
   "metadata": {},
   "source": [
    "Load the parameters, libraries and datasets needed for the federated training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sticky-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "# Federated imports\n",
    "import forcast_federated_learning as ffl\n",
    "\n",
    "# Parameters\n",
    "num_clients        = 10\n",
    "com_rounds         = 20\n",
    "seed               = 0\n",
    "batch_size         = 200\n",
    "noise_multiplier   = 0.2\n",
    "max_grad_norm      = 0.5\n",
    "epochs             = 2\n",
    "lr                 = 0.005\n",
    "device             = 'cuda' # 'cpu'\n",
    "\n",
    "# Metrics\n",
    "df_metrics = pd.DataFrame(dict(zip(['round', 'accuracy', 'loss', 'epsilon', 'delta'], [int,[],[],[],[]])))\n",
    "\n",
    "# Load local train data\n",
    "import tensorflow as tf\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Create custom pytorch datasers for train and testing\n",
    "traindata = ffl.datasets.ImageDataset(X_train, y_train, categorical=True)\n",
    "testdata  = ffl.datasets.ImageDataset(X_test, y_test, categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-march",
   "metadata": {},
   "source": [
    "Split the train data between the clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handed-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train data and use only a fraction\n",
    "traindata_split = ffl.data.random_split(traindata, num_clients=num_clients, seed=seed)\n",
    "\n",
    "# Get data loader\n",
    "train_loaders = [ffl.utils.DataLoader(traindata, batch_size=batch_size, shuffle=True, seed=seed)   for traindata in traindata_split]\n",
    "test_loader  = ffl.utils.DataLoader(testdata, batch_size=len(testdata), shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "double-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d( 1, 64, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.bn1   = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        # flatten\n",
    "        self.bn2   = nn.BatchNorm1d(128 * 4 * 4)\n",
    "        self.fc1   = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2   = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-format",
   "metadata": {},
   "source": [
    "Initialize every model for each client. The model topology and optimizer is shared between the clients model and the server or global model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "consolidated-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train params\n",
    "optimizer_params   = {'lr': lr}\n",
    "train_params       = {'epochs': epochs}\n",
    "\n",
    "local_models       = []\n",
    "for _ in range(num_clients):\n",
    "    # Create federated model based on a pytorch model\n",
    "    num_features, num_classes  = 4, 3\n",
    "    model                      = CNN() # pytorch model\n",
    "    loss_fn                    = nn.CrossEntropyLoss() # classification\n",
    "    local_model                = ffl.LocalModel(model, model_type = 'nn', loss_fn=loss_fn, train_params=train_params)\n",
    "    local_model.optimizer      = ffl.optim.Adam(local_model.parameters(), **optimizer_params)\n",
    "    \n",
    "    local_models.append(local_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-strength",
   "metadata": {},
   "source": [
    "Initialize the global model  and in case is needed the encryption parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "organized-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "model           = local_model.model # pytorch model\n",
    "fed_model       = ffl.FederatedModel(model, model_type='nn')\n",
    "public_context, secret_key = ffl.encryption.get_context()\n",
    "encryption      = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-prescription",
   "metadata": {},
   "source": [
    "Each round of communication every client individually train their respective model using private data. Then aggregate those models onto the global model. That model is then shared to the clients and the cicle repeats until a model with sufficient accuracy is obtained or the a fixed number of communication rounds in reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "swiss-garden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ddf24618a2443495a5bc1ae0dffb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 11.35\n",
      "Test accuracy: 71.65\n",
      "Test accuracy: 86.37\n",
      "Test accuracy: 93.85\n",
      "Test accuracy: 94.31\n",
      "Test accuracy: 97.09\n",
      "Test accuracy: 96.69\n",
      "Test accuracy: 97.61\n",
      "Test accuracy: 97.71\n",
      "Test accuracy: 98.06\n",
      "Test accuracy: 98.38\n",
      "Test accuracy: 97.53\n",
      "Test accuracy: 98.08\n",
      "Test accuracy: 98.59\n",
      "Test accuracy: 98.46\n",
      "Test accuracy: 98.38\n",
      "Test accuracy: 98.67\n",
      "Test accuracy: 98.36\n",
      "Test accuracy: 98.44\n",
      "Test accuracy: 98.72\n"
     ]
    }
   ],
   "source": [
    "for com_round in tqdm(range(com_rounds)):\n",
    "    for local_model, train_loader in zip(local_models, train_loaders):\n",
    "        local_model.step(train_loader, device=device)\n",
    "    \n",
    "    client_weights = []\n",
    "    for local_model in local_models:\n",
    "        state_dict      = local_model.state_dict()\n",
    "        if encryption:\n",
    "            # Each client encrypts the their model parameters (state_dict)\n",
    "            # The library handles internally the encrypted data so the functions don't change much\n",
    "            enc_state_dict  = ffl.encryption.EncStateDict(state_dict)\n",
    "            enc_state_dict  = enc_state_dict.encrypt(public_context)\n",
    "            client_weights.append(enc_state_dict)\n",
    "        else:\n",
    "            client_weights.append(state_dict)\n",
    "    client_lens    = [len(traindata) for traindata in traindata_split]\n",
    "    \n",
    "    ## Server aggregate\n",
    "    if encryption:\n",
    "        fed_model.server_agregate(client_weights, client_lens, secret_key=secret_key)\n",
    "    else:\n",
    "        fed_model.server_agregate(client_weights, client_lens)\n",
    "    weights = fed_model.state_dict()\n",
    "    \n",
    "    for local_model in local_models:\n",
    "        local_model.load_state_dict(weights)\n",
    "    \n",
    "    acc, loss = local_model.test(test_loader) # local model now is the same as the global model\n",
    "    print(f'Test accuracy: {acc:.2f}')\n",
    "    df_aux       = pd.DataFrame({'round': [com_round+1], 'accuracy': [acc], 'loss': [loss], 'epsilon': [None], 'delta':[None] })\n",
    "        \n",
    "    # Save metrics\n",
    "    df_metrics   = pd.concat([df_metrics, df_aux], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interstate-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save metrics onto csv file\n",
    "# df_metrics.to_csv('./sim_mnist.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-mentor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
